# AI-Driven Pairs Trading with Deep Reinforcement Learning

## TLDR

* Developed a Deep Reinforcement Learning (PPO) trading agent that turned a -63% baseline loss into a positive return on out-of-sample data.

* Engineered a custom gymnasium environment with added features (volatility, momentum) to train an adaptive AI for a dynamic pairs trading strategy.

## Overview

This project implements a sophisticated pairs trading strategy for the PEP (pepsi)/KO (Coca Cola) stock pair, building upon classical quantitative finance principles from the book *Successful Algorithmic Trading* and enhancing them with a modern Deep Reinforcement Learning (DRL) agent.

The core idea is to move beyond static, rules-based trading (e.g., "trade when the z-score is > 2") and empower an AI agent to learn a dynamic, adaptive trading policy. The agent is trained to maximize its risk-adjusted returns by observing market conditions—including the pair's spread, volatility, and momentum—and deciding the optimal time to enter, exit, or hold a position.

The final optimized RL agent successfully learns a profitable strategy on out-of-sample data, demonstrating a significant improvement over the unprofitable rules-based baseline.

### Key Concepts Implemented
* **Cointegration & Stationarity:** Statistical validation of the pairs relationship using the Cointegrated Augmented Dickey-Fuller (CADF) test.
* **Mean-Reversion Strategy:** Implementation of a baseline z-score-based trading strategy for benchmarking.
* **Feature Engineering:** Creation of market context features (volatility, momentum) to improve the agent's decision-making.
* **Reinforcement Learning Environment:** A custom `gymnasium` environment that simulates the trading of the pair spread.
* **Hyperparameter Tuning:** Automated optimization of the RL agent's parameters using `Optuna` to maximize out-of-sample Sharpe Ratio.
* **Agent Training:** Training a Proximal Policy Optimization (PPO) agent from `stable-baselines3` to learn the trading policy.
* **Rigorous Backtesting:** Out-of-sample (walk-forward) validation to ensure the strategy is robust and not overfit.

## Project Structure

* `AI_Pairs_Trading.ipynb`: The main Jupyter Notebook that walks through the entire project, from data collection to the final evaluation of the optimized agent.
* `rl_environment.py`: A Python script defining the custom `PairsTradingEnv` for the RL agent. This is imported by the notebook.
* `requirements.txt`: A list of all the necessary Python libraries to run the project.
* `pep_ko_data.csv`: The raw data file generated by the notebook in Step 1.
* `baseline_pep_ko_backtest.csv`: The backtest results for the baseline strategy, generated in Step 2.

## How to Run

1.  **Clone the Repository:**
    ```bash
    git clone <your-repo-url>
    cd <your-repo-name>
    ```

2.  **Set up a Virtual Environment (Recommended):**
    ```bash
    python -m venv env
    source env/bin/activate  # On Windows, use `env\Scripts\activate`
    ```

3.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Run the Jupyter Notebook:**
    Launch Jupyter and open the `AI_Pairs_Trading.ipynb` file.
    ```bash
    jupyter notebook
    ```
    Execute the cells in the notebook from top to bottom. **Note:** The hyperparameter optimization cell in Step 4 will take a significant amount of time to run (20-30+ minutes).

## Results

The project successfully demonstrates the value of an adaptive AI approach. While the simple rules-based strategy was unprofitable on the 2018-2019 test data, the final optimized Reinforcement Learning agent achieved a **positive Total Return and Sharpe Ratio**, learning to navigate the volatile market conditions effectively.

<img width="993" height="578" alt="image" src="https://github.com/user-attachments/assets/f468247c-f26f-48ea-94c8-5d460ce505bf" />

